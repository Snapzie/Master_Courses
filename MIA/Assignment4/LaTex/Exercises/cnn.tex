\section{Basics of CNNs}
Convolutional neural networks are a sub class of deep neural networks which mainly consists convolutional layers. A convolution uses a kernel which slides across the image and performs a weighted sum over all affected pixels in the image. Kernel sizes of 3 by 3 or 5 by 5 are the most common. The output of the convolution is a new image which is smaller in size than the original image. To overcome this, a number of different padding strategies can be used, the most common is to pad the original image with zeros such that the output of the convolution has the same size as the input image. Different kernels produces different outputs, for instance putting equal weight in each kernel index will blur the original image as the result of using such kernel is the average pixel value in a local neighbourhood. Defining a 3 by 3 kernel where the top row are ones, the middle are zeros and the last are negative ones will find horizontal lines where we have background below the edge.\\
Convolutions can find patterns in a local neighbourhood, but not in a global setting, thus max pooling layers are often used in CNNs to shrink the size of the image but to keep the important features found by the convolutions. Max pooling works by taking the max value in a local image patch, often a 2 by 2 patch. Performing a sequence of convolution(s) followed by a max pooling will result in the image shrinking and the convolutions in a sense can find features at bigger and bigger scale as the relative area of the image the kernel operates increases when the image shrinks. This makes CNNs scale invariant and allows for object detection regardless of the object being in the corner of the image, or if it fills the whole image.\\
Each convolutional in the network detects a certain feature, this could be diagonal lines or round object. As we at some point need to Linearly combine these responses to make a prediction based on the responses of the convolutions, we are not interested in for instance how 'not round' an object is. For this reason we use activation functions. These are non-linear functions like ReLU which removes negative responses but keeps positive responses. Another example is the sigmoid function which gives a probability. Using activation functions ensures the 'not roundness' of an object does not skew the result of the network as we are only interested in if the object is round. 