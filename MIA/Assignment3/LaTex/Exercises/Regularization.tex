\section{Regularization}
Regularization is used as extra terms in the optimization problem to penalize unwanted behaviour. If we simply optimize without regularization it might be we get a decent transformation at all points except one, where it lies very far away from where we would expect. In that scenario we can add a regularization term which takes the difference in \textit{x} and \textit{y} coordinates before and after the transformation and squares this distance and adds it or subtracts it (depending on our optimization problem) from our optimization to penalize single major errors. This effectively makes the algorithm prefer many small errors in our transformation rather than few major errors. Another example of a regularization term could be to penalize folding as a solution to minimize distances. This can be done by adding a regularization terms which add / subtracts from our optimization depending on the sign of the transformation. Many more regularizations exists, and as long as they add or subtract from the optimization they can be used as regularization terms. Of course it is possible to over regularize an optimization, in which case the optimal solution might simply become to not do any transformation as this is less punishing than transforming the image, and so, one must be careful not to make the optimization too restrictive.