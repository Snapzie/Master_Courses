\section{Conclusion}
In conclusion we saw during the model selection, that a more diverse and larger training set was not necessarily the key factors to achieve good results as we would have expected. Instead it was a model trained on videos from the same patient which was deemed the best performing model. It is hard to conclude why this is, but it is likely due to a combination of the training data being diverse, but not overly diverse, while the training data not containing overly many noisy frames. However, this model was, together with the others, heavily overfitting and more regularization should have been used during training. Nonetheless, this model had $65\%$ validation accuracy, but overly predicting inflammation when visually inspecting its performance on unseen videos. Due to the low accuracy and bias towards predicting inflammation, we thus conclude the model in its current state, is too unreliable to aid a doctor in the diagnosis of patients.\\
To predict the separation points, two post-processing approaches were used. The Random Walker approach had a tendency to be very conservative and predict a separation point close to the end of the video. This is likely due to the walker getting 'stuck' on noise, but with noise inherently being part of the videos, it is still questionable if this approach can produce meaningful results. An evaluation on all the videos showed it on average predicted a separation point $51.85\%$ of the total frames in the video away from the true separation point, although the variance being high, as some predictions were very accurate and others not. Based on these results, we conclude the Random Walker approach is not accurate enough to aid a doctor in the diagnosis of colitis or in the processing of endoscopy videos.\\
Looking at the scoring heuristic, evaluation on all the videos showed it on average predicted a separation point $51.87\%$ of the total frames in the video away from the true separation point. It however had less variance in its predictions, as this approach was not as conservative as the Random Walker, but also not as accurate when performing well. The fact this approach provides candidate points, and at least one of these candidate points often is near the true separation point, could be beneficial for a doctor. However, because it is not always true a candidate point is close, and it being unknown which candidate point is close, it is not of much help in practice. Thus we conclude this approach is not suited for separation point prediction.\\
Performing treatment predictions only on the predicted segmentations from model \textit{Idx\_2\_3\_4\_5\_6\_skip\_20} yielded $40\%$ accuracy, however, the predictions consisted only of classes 0 and 2. Although $40\%$ accuracy is decent in a 5 class classification problem, due to the predictions only consisting of 2 classes, the false positive rate is high, which was also validated in the ROC curve. However, both class 0, 1 and the micro-average achieved an area under the curve a fair bit above $0.5$, indicating the model to some extent is capable of distinguishing between the classes. When adding the true segmentations to the training, the accuracy increased to $48\%$ although still only two classes were predicted, and we saw similar results in the ROC curves. Thus we conclude the treatment predictions works to some extent, but the accuracy is too low to be of aid to a doctor.\\
Training a U-Net to give the original segmentations some more structure, and bring them closer to the true segmentations, to some extent worked as a lot of frames classified with inflammation got overturned to healthy. However, overly many healthy frames were predicted, likely as a reaction to model \textit{Idx\_2\_3\_4\_5\_6\_skip\_20} overly predicting inflammation. In conclusion, the U-Net seemed reluctant to predict inflammation, but overall, the segmentations seemed to become more 'blocky' but only slightly more accurate.\\
To end with, treatment predictions were made on the results of the U-Net segmentations. Using both predicted and true segmentations, $20\%$ accuracy was achieved, and class 0,1 and 2 were predicted. The ROC curve only shows class 0 having an area under the curve above  $0.5$, and thus it is no surprise the accuracy is low. When only training on the predicted segmentations from the U-Net, we achieve $52\%$ accuracy while predicting classes 0,1 and 2. The ROC curve shows, however, all classes being below $0.53$, which indicates although the model makes the correct predictions, it is not very confident in its predictions, and it is likely it has close to equal probabilities for each class. In conclusion $52\%$ accuracy is fairly good on a 5 class classification task, but again, it is not reliable enough to function as a second opinion for a doctor.